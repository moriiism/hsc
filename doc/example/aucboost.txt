#
# aucboost.txt
#

Analysis by AUC boosting

# Input data
To train the AUC boosting machine, users must prepare an input data file
with the CSV format. Rows and columns of the file correspond to sources
(astronomical objects) and features of the sources.
The file must also contain a column of real/bogus flag.
In the following example, the input file is
"fake31822_4sigma_20160512_head1000.csv".

   format: CSV format
   columns: num,patch,mag,magerr,elongation_norm,fwhm_norm,
            significance_abs,residual,psffit_sigma_ratio,
            psffit_peak_ratio,frac_det,density,density_good,
            bapsf,psf,score
            (num: index from one: 1, 2, 3, ...)
            Details of the meaning of the features are shown in
            M. Morii et al. (2016) PASJ, 68, 104.
   number of rows: 999

   # "num" and "patch" are not used in the following process.

# There are 5 Steps:
# 0-th step is to make random partitioned data.
# 1st and 2nd steps are to make an averaged ROC curve 
# to check the performance of AUC boosting.
# 3rd and 4th steps are to make a trained machine.
# 1st and 2nd steps can be skipped, if you need not evaluate the ROC curve.

0. Random partitioning step
   Make N (for example, 30) data sets by randomly partitioning
   the ID list of the input data.
   Each data set is made of three lists of the ID
   (training data, validation data, test data),
   with (nearly) equal numbers of entries.

   For example, 
   cv[00-29]-te.csv         : test data (30 files)
   cv[00-29]-[00-29]-tr.csv : training data (30 x 30 files)
   cv[00-29]-[00-29]-vl.csv : validation data (30 x 30 files)

   (cv00-00-tr.csv + cv00-00-vl.csv), (cv00-01-tr.csv + cv00-01-vl.csv), ...,  
   (cv00-29-tr.csv + cv00-29-vl.csv) are the same. They are complementary set of cv01-te.csv.

   These data (training data, validation data, test data) are used differently
   in two contexts. One is to make a trained machine and the other is to make an averaged ROC curve.
   In the former context (to make a trained machine), cv00-00-tr.csv and cv00-00-vl.csv data are
   merged (cv00-00-tr.csv + cv00-00-vl.csv) and used for the training data.
   The corrsponding cv00-te.csv is used for the test data.
   In the latter context (to make an averaged ROC curve), cv00-00-tr.csv is used for training,
   cv00-00-vl.csv is used for validation, and cv00-te.csv is used for test.

1. To make an averaged ROC curve:
   A machine is trained by using a training data (cv00-00-tr.csv) with hyper-parameter (lambda)
   values varied (For example, 10 points from 1.e-5 to 1.e+2 in log scale).
   The performance of the 10 trained machines with different lambdas
   is evaluated by applying these machines for the corresponding validation data (cv00-00-vl.csv), 
   where the performance is evaluated by the AUC value.
   The best lambda value among the 10 lambda values is determined.
   This process is repeated in 30 pairs of training and validation data: (cv00-00-tr.csv, cv00-00-vl.csv),
   (cv00-01-tr.csv, cv00-01-vl.csv), ..., (cv00-29-tr.csv, cv00-29-vl.csv),
   so 30 lambda values are obtained. Then, the averaged lambda value is calculated.
   This lambda value is the best lambda value for the complementary data set of cv00-te.csv,
   that is for example (cv00-00-tr.csv + cv00-00-vl.csv).
   Furthermore, the above process is repeated in the 30 partitions:
   (cv00-te.csv, (cv00-[00-29]-tr.csv, cv00-[00-29]-vl.csv) ),
   (cv01-te.csv, (cv01-[00-29]-tr.csv, cv01-[00-29]-vl.csv) ), ...,
   (cv29-te.csv, (cv29-[00-29]-tr.csv, cv29-[00-29]-vl.csv) ).
   For each partition, the best lambda value can be different.

2. To make an averaged ROC curve:
   For each partition, a machine is trained by using (cv00-00-tr.csv + cv00-00-vl.csv)
   with the lambda value fixed to the best lambda determined in the previous step.
   The performance of the trained machines is evaluated by applying the machine
   for the corresponding test data (cv00-te.csv). Then, one ROC curve is obtained for
   each partition. This process is repeated in the 30 partitions, 
   then 30 ROC curves are obtained. Finally, the averaged ROC curve can be obtained.

   For each partition, the process shown in the following step 3 and 4 is simulated.
   The final ROC curve corresponds to the ensemble of such 30 simulations.

3. To make a trained machine:
   For each partition, a machine is trained by using training+validation data
   with lambda values varied (For example, 10 points from 1.e-5 to 1.e+2 in log scale).
   The performance of the 10 trained machines with different lambdas is evaluated
   by applying these machines for the corresponding test data (cv00-te.csv),
   where the performance is evaluated by the AUC value.
   The best lambda value among the 10 lambda values is determined.
   This process is repeated in 30 partitions, so 30 lambda values are obtained. 
   Then, the averaged lambda value is calculated as the best lambda value.

4. To make a trained machine:
   The final trained machine is made by using all the data as the training data
   by setting the lambda to the best lambda obtained by the 3rd step.
   The final trained machine is indeed a file containing parameters.
   This parameter file is used as the input parameter file of the python script
   (calc_score_th.py) in TrainedMachine directory.


###
### example
###

cd       /home/morii/work/hsc/ana
mkdir -p 19032501
cd       19032501

mkdir data
cp fake31822_4sigma_20160512_head1000.csv \
   data/fake31822_4sigma_20160512_head1000.csv

###
### step0
###

### Make random sample list from the input csvfile

csvfile=data/fake31822_4sigma_20160512_head1000.csv
csv_version=16051900
## csv_version=17112400
nset=30
outdir=fake31822
seed=0

/home/morii/work/github/moriiism/hsc/step/step0/step0 \
$csvfile \
$csv_version \
$nset \
$outdir \
$seed

---> output
fake31822/cv??-te.csv : test data
fake31822/cv??-??-tr.csv : training data
fake31822/cv??-??-vl.csv : validation data

###
### step1: to make an averaged ROC curve
###

### training and validation

mkdir setup

csvfile=data/fake31822_4sigma_20160512_head1000.csv
csv_version=16051900
## csv_version=17112400
source_id_dir=fake31822
lambda_dat=setup/lambda.dat
niter=150
nset=30
outdir=trvl_step1

cat << EOF > setup/lambda.dat
# npoint  lo  up  scale
10   1.e-5  1.e2   log
EOF


/home/morii/work/github/moriiism/hsc/step/step1/step1 \
$csvfile \
$csv_version \
$source_id_dir \
$lambda_dat \
$niter \
$nset \
$outdir

--->  best lambda values for test data are written in trvl_step1/lambda.dat

###
### step2 : to make an averaged ROC curve
###

# Train machines by setting hyper parameters of lambda for each test data,
# using train+validate_data
# and evaluate the trained machine by test-data

csvfile=data/fake31822_4sigma_20160512_head1000.csv
csv_version=16051900
## csv_version=17112400
source_id_dir=fake31822
lambda_file=trvl_step1/lambda.dat
niter=150
nset=30
outdir=trvl_step2

/home/morii/work/github/moriiism/hsc/step/step2/step2 \
$csvfile \
$csv_version \
$source_id_dir \
$lambda_file \
$niter \
$nset \
$outdir


###
### step3 : to make a trained machine
###

### Calculate the best hyper parameter of lambda

csvfile=data/fake31822_4sigma_20160512_head1000.csv
csv_version=16051900
## csv_version=17112400
source_id_dir=fake31822
lambda_dat=setup/lambda.dat
niter=150
nset=30
outdir=trvl_step3

cat << EOF > setup/lambda.dat
# npoint  lo  up  scale
5   1.e-5  1.e2   log
EOF

/home/morii/work/github/moriiism/hsc/step/step3/step3 \
$csvfile \
$csv_version \
$source_id_dir \
$lambda_dat \
$niter \
$nset \
$outdir


The best hyper-parameter lambda is written in
trvl_step3/lambda.dat

###
### step4 : to make a trained machine
###

# Make the final trained machine by using all the data
# by setting the hyper parameter of lambda

csvfile=data/fake31822_4sigma_20160512_head1000.csv
csv_version=16051900
## csv_version=17112400
lambda=5.011872336e-05
niter=150
outdir=trvl_step4

/home/morii/work/github/moriiism/hsc/step/step4/step4 \
$csvfile \
$csv_version \
$lambda \
$niter \
$outdir

# output
trained parameter file to be used the trained machine:
trvl_step4/boosting_par.dat


# copy the trained parameter file to TrainedMachine

cp <trained_parameter_file>  >  \
/home/morii/work/github/moriiism/hsc/TrainedMachine/trained.dat

# run the trained machine

/home/morii/work/github/moriiism/hsc/TrainedMachine/calc_score_th.py \
/home/morii/work/github/moriiism/hsc/TrainedMachine/trained_16070700.dat \
<input.csv.file>  \
<output.file>  \
<threshold>


###
### more detailed explanation of the above processes.
###

orginal data: fake31822_4sigma_20160512_head1000.csv
              contains columns of features and real/bogus flag,
              and contains 999 rows.

0th step, random partitioning step, produces the following
partitioned data sets.

  test data   validation data   training data
  333 rows         333 rows       333 rows
cv00-te.csv -- cv00-00-vl.csv  cv00-00-tr.csv
               cv00-01-vl.csv  cv00-01-tr.csv
                     :               :
                     :               :
               cv00-29-vl.csv  cv00-29-tr.csv

cv01-te.csv -- cv01-00-vl.csv  cv01-00-tr.csv
               cv01-01-vl.csv  cv01-01-tr.csv
                     :               :
                     :               :
               cv01-29-vl.csv  cv01-29-tr.csv
    :       
    :
cv29-te.csv -- cv29-00-vl.csv  cv29-00-tr.csv
               cv29-01-vl.csv  cv29-01-tr.csv
                     :               :
                     :               :
               cv29-29-vl.csv  cv29-29-tr.csv

3rd and 4th steps produce a trained machine.
One lambda value is determined in 3rd step as follows:
             training                 test
(cv00-00-vl.csv + cv00-00-tr.csv), cv00-te.csv ----> lambda_00 --+
(cv01-00-vl.csv + cv01-00-tr.csv), cv01-te.csv ----> lambda_01 --+
                                :                        :       +--(average)--> lambda_best
                                :                        :       |
(cv29-00-vl.csv + cv29-00-tr.csv), cv29-te.csv ----> lambda_29 --+

4th step uses the lambda_best value. It use all the data as the training data:
(cv00-te.csv + cv00-0f0-vl.csv + cv00-00-tr.csv).

1st and 2nd steps simulate the 3rd and 4th steps in every 30 partition.
One lambda value is determined in 1st step as follows:
    training         test 
cv00-00-tr.csv, cv00-00-vl.csv ----> lambda_00_00 --+
cv00-01-tr.csv, cv00-01-vl.csv ----> lambda_00_01 --+
              :                         :           +--(average)--> lambda_best_00
              :                         :           |
cv00-29-tr.csv, cv00-29-vl.csv ----> lambda_00_29 --+

cv01-00-tr.csv, cv01-00-vl.csv ----> lambda_01_00 --+
cv01-01-tr.csv, cv01-01-vl.csv ----> lambda_01_01 --+
              :                         :           +--(average)--> lambda_best_02
              :                         :           |
cv01-29-tr.csv, cv01-29-vl.csv ----> lambda_01_29 --+
                        :
                        :
cv29-00-tr.csv, cv29-00-vl.csv ----> lambda_29_00 --+
cv29-01-tr.csv, cv29-01-vl.csv ----> lambda_29_01 --+
              :                         :           +--(average)--> lambda_best_29
              :                         :           |
cv29-29-tr.csv, cv29-29-vl.csv ----> lambda_29_29 --+

30 ROC curves and an averaged ROC curve are made as follows:
            training                  test
(cv00-00-tr.csv + cv00-00-vl.csv), cv00-te.csv, using lambda_best_00  ---> ROC_curve_00 --+
(cv01-00-tr.csv + cv01-00-vl.csv), cv01-te.csv, using lambda_best_01  ---> ROC_curve_01 --+
                               :                                               :          +-- (average)--> ROC curve
                               :                                               :          |
(cv29-00-tr.csv + cv29-00-vl.csv), cv29-te.csv, using lambda_best_29  ---> ROC_curve_29 --+

